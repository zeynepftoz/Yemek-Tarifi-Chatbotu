import os
import pandas as pd
import streamlit as st
from dotenv import load_dotenv

from langchain_core.documents import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import Chroma
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    raise ValueError("Google API key bulunamadÄ±. .env dosyasÄ±nÄ± kontrol et.")

# --- Excel dosyasÄ±nÄ± oku
tarif_df = pd.read_excel("yemekchatbot_200ornek.xlsx")  # Excel dosya adÄ±nÄ± kendine gÃ¶re deÄŸiÅŸtir

# --- Sayfa baÅŸlÄ±ÄŸÄ± ve aÃ§Ä±klama
st.title("ğŸ² Yemek Tarifi AsistanÄ±")
st.markdown("""
ğŸ‘‹ **HoÅŸ geldin!** AÅŸaÄŸÄ±ya yemek tarifleriyle ilgili bir soru yazarak tarif arayabilirsin.  
Ã–rnek sorular:
- "Kolay ve hÄ±zlÄ± bir tatlÄ± Ã¶nerir misin?"
- "Baklava tarifi nasÄ±l yapÄ±lÄ±r?"
- "Malzemeleri ÅŸeker ve ceviz olan tatlÄ±lar neler?"
- "HazÄ±rlamasÄ± 30 dakikadan kÄ±sa sÃ¼ren yemek tarifleri var mÄ±?"
---
""")

# --- Excel'den veri -> Document
def yemek_to_document(row):
    text = f"""Yemek AdÄ±: {row['Yemek AdÄ±']}
SÃ¼re (dk): {row['SÃ¼re (dk)']}
Zorluk: {row['Zorluk']}
Malzemeler: {row['Malzemeler']}
Tarif: {row['Tarif']}"""
    return Document(page_content=text)

docs = [yemek_to_document(row) for _, row in tarif_df.iterrows()]
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
split_docs = splitter.split_documents(docs)

# --- VektÃ¶r veritabanÄ±
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")
vectorstore = Chroma.from_documents(
    documents=split_docs,
    embedding=embeddings,
    persist_directory="./chroma_yemek_db"
)
retriever = vectorstore.as_retriever(search_type="similarity", search_kwargs={"k": 5})

# --- LLM tanÄ±mÄ±
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-pro",
    temperature=0.3,
    max_tokens=500
)

# --- Intent sÄ±nÄ±flandÄ±rma zinciri
intent_prompt = PromptTemplate.from_template("""
AÅŸaÄŸÄ±daki kullanÄ±cÄ± cÃ¼mlesinin hangi intent'e ait olduÄŸunu belirle. 
Intent kategorileri:
- Yemek Ã–nerisi
- Malzeme Sorgusu
- SÃ¼re Sorgusu
- Zorluk Sorgusu
- Selamlama
- VedalaÅŸma
- Bilinmeyen

CÃ¼mle: {soru}
Intent:
""")
intent_chain = intent_prompt | llm | StrOutputParser()

# --- RAG prompt
system_prompt = (
    "Sen yemek tarifleri hakkÄ±nda bilgi veren bir asistansÄ±n.\n"
    "AÅŸaÄŸÄ±daki tarifleri kullanarak kullanÄ±cÄ± sorusunu yanÄ±tla.\n"
    "CevabÄ±n kÄ±sa, net ve baÄŸlama dayalÄ± olsun. Bilinmeyen bir ÅŸey varsa 'emin deÄŸilim' de.\n\n"
    "{context}"
)
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", system_prompt),
    ("human", "{input}")
])

# --- KullanÄ±cÄ±dan input al
query = st.text_input("ğŸ½ï¸ Yemek tarifleriyle ilgili bir sorunuz varsa buraya yazÄ±n:")

if query:
    intent = intent_chain.invoke({"soru": query})
    st.markdown(f"**ğŸ¯ Intent tespiti:** `{intent}`")

    if intent.lower() in ["yemek Ã¶nerisi", "malzeme sorgusu", "sÃ¼re sorgusu", "zorluk sorgusu"]:
        question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
        rag_chain = create_retrieval_chain(retriever, question_answer_chain)
        response = rag_chain.invoke({"input": query})
        st.write(response["answer"])

    elif intent.lower() == "selamlama":
        st.write("Merhaba! Yemek tarifleri konusunda size nasÄ±l yardÄ±mcÄ± olabilirim? ğŸ³")

    elif intent.lower() == "vedalaÅŸma":
        st.write("Afiyet olsun! BaÅŸka sorularÄ±nÄ±z olursa beklerim. ğŸ‘‹")

    else:
        st.write("Sorunuzu anlayamadÄ±m. LÃ¼tfen daha aÃ§Ä±k bir ÅŸekilde yazabilir misiniz?")
